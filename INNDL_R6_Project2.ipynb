{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4WH1Pr4KQlCh"
   },
   "source": [
    "### Problem:\n",
    "\n",
    "Recognizing multi-digit numbers in photographs captured at street level is an important component of modern-day map making. A classic example of a corpus of such street level photographs is Googleâ€™s Street View imagery comprised of hundreds of millions of geo-located 360 degree panoramic images. The ability to automatically transcribe an address number from a geo-located patch of pixels and associate the transcribed number with a known street address helps pinpoint, with a high degree of accuracy, the location of the building it represents. More broadly, recognizing numbers in photographs is a problem of interest to the optical character recognition community. While OCR on constrained domains like document processing is well studied, arbitrary multi-character text recognition in photographs is still highly challenging. This difficulty arises due to the wide variability in the visual appearance of text in the wild on account of a large range of fonts, colors, styles, orientations, and character arrangements. The recognition problem is further complicated by environmental factors such as lighting, shadows, specularities, and occlusions as well as by image acquisition factors such as resolution, motion, and focus blurs.\n",
    "\n",
    "In this project we will use dataset with images centred around a single digit (many of the images do contain some distractors at the sides). Although we are taking a sample of the data which is simpler, it is more complex than MNIST because of the distractors.\n",
    "\n",
    "Link to dataset:\n",
    "https://drive.google.com/file/d/1L2-WXzguhUsCArrFUc8EEkXcj33pahoS/view?usp=sharing\n",
    "\n",
    "\n",
    "The objective of the project is to learn how to implement a simple image classification pipeline based on a deep neural network. The goals of this project are as follows: \n",
    "- Understand the basic Image Classification pipeline and the data-driven approach (train/predict stages) \n",
    "- Data fetching and understand the train/val/test splits. (5 points)\n",
    "- Implement and apply a deep neural network classifier including (feedforward neural network, RELU, activations) (10 points)\n",
    "- Understand and be able to implement (vectorized) backpropagation (cost stochastic gradient descent, cross entropy loss, cost functions) (10 points)\n",
    "- Implement batch normalization for training the neural network (5 points) \n",
    "- Print the classification accuracy metrics (5 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TbvI8LqlQlCl",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SPW-a-qYQlCp"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "74cQBsi5QlCw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1. Data fetching and understand the train/val/test splits. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = h5py.File('Data/SVHN_single_grey1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['X_test', 'X_train', 'X_val', 'y_test', 'y_train', 'y_val']>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(data_raw.get('X_train'))\n",
    "X_test = np.array(data_raw.get('X_test'))\n",
    "X_val = np.array(data_raw.get('X_val'))\n",
    "y_train = np.array(data_raw.get('y_train'))\n",
    "y_test = np.array(data_raw.get('y_test'))\n",
    "y_val = np.array(data_raw.get('y_val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42000, 32, 32),\n",
       " (18000, 32, 32),\n",
       " (60000, 32, 32),\n",
       " (42000,),\n",
       " (18000,),\n",
       " (60000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, X_val.shape, y_train.shape, y_test.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's review the data\n",
    "img_num = np.random.randint(0, X_test.shape[0]) #Get a random integer between 0 and number of examples in test dataset\n",
    "plt.imshow(X_test[img_num],cmap='gray') #Show the image from test dataset\n",
    "plt.suptitle('Number: ' + str(y_test[img_num]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement and apply a deep neural network classifier including (feedforward neural network, RELU, activations) (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_enc = tf.convert_to_tensor(tf.keras.utils.to_categorical(y_train, num_classes=10))\n",
    "y_test_enc = tf.convert_to_tensor(tf.keras.utils.to_categorical(y_test, num_classes=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = X_train / 255.0\n",
    "X_test_norm = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reshaped = tf.keras.backend.expand_dims(X_train_norm)\n",
    "X_test_reshaped = tf.keras.backend.expand_dims(X_test_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some notes from: https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/\n",
    "- Number of filters should be increasing as we go from input to output layer\n",
    "- Number of filters should be a power of 2\n",
    "- MaxPooling should be used to reduce the spatial dimension of the output layer\n",
    "- The filter_size should be of odd integer number i.e. 3x3 or 5x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the session before building the model\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Initialize model, reshape & normalize data\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#Normalize the data\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "#Add first convolutional layer\n",
    "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Add second convolutional layer\n",
    "model.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Add third convolutional layer\n",
    "model.add(tf.keras.layers.Conv2D(128, kernel_size=(5,5), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Flatten the output\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#First Dense layer\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second Dense layer\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Understand and be able to implement (vectorized) backpropagation (cost stochastic gradient descent, cross entropy loss, cost functions) (10 points)\n",
    "- After various trials, here I have used \"adam\" optimizer than \"sgd\" as it gave higher accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement batch normalization for training the neural network (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 92s 2ms/sample - loss: 0.6891 - accuracy: 0.7882 - val_loss: 0.4322 - val_accuracy: 0.8743\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 90s 2ms/sample - loss: 0.3461 - accuracy: 0.8999 - val_loss: 0.3588 - val_accuracy: 0.8993\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 92s 2ms/sample - loss: 0.2732 - accuracy: 0.9208 - val_loss: 0.3411 - val_accuracy: 0.8985\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 89s 2ms/sample - loss: 0.2297 - accuracy: 0.9327 - val_loss: 0.3107 - val_accuracy: 0.9117\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 89s 2ms/sample - loss: 0.1917 - accuracy: 0.9446 - val_loss: 0.3172 - val_accuracy: 0.9122\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 89s 2ms/sample - loss: 0.1641 - accuracy: 0.9503 - val_loss: 0.3232 - val_accuracy: 0.9129\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 90s 2ms/sample - loss: 0.1351 - accuracy: 0.9601 - val_loss: 0.3359 - val_accuracy: 0.9154\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 94s 2ms/sample - loss: 0.1165 - accuracy: 0.9648 - val_loss: 0.3465 - val_accuracy: 0.9186\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 90s 2ms/sample - loss: 0.1000 - accuracy: 0.9690 - val_loss: 0.3792 - val_accuracy: 0.9163\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 88s 2ms/sample - loss: 0.0868 - accuracy: 0.9720 - val_loss: 0.3950 - val_accuracy: 0.9132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x164f1494a48>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_reshaped, y_train_enc, validation_data=(X_test_reshaped, y_test_enc), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Print the classification accuracy metrics (5 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_number = [np.argmax(row) for row in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of the model: 91.32%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy Score of the model: %.2f%%' % (accuracy_score(y_test, y_pred_number)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1658,   59,   17,    6,   13,    3,   10,    7,    7,   34],\n",
       "       [  15, 1682,    8,   22,   40,   11,    2,   27,   11,   10],\n",
       "       [   3,   17, 1675,   33,   14,   10,    4,   20,    5,   22],\n",
       "       [   6,   26,   14, 1551,    9,   63,    8,   13,   14,   15],\n",
       "       [   6,   40,   23,   14, 1693,    6,    9,    8,    4,    9],\n",
       "       [   1,   13,    4,   44,   17, 1658,    9,    2,   10,   10],\n",
       "       [  18,   16,   13,   19,   16,   86, 1609,    3,   43,    9],\n",
       "       [   6,   45,   24,   25,   12,    5,    1, 1680,    4,    6],\n",
       "       [  16,   33,   18,   45,    7,   19,   40,    8, 1587,   39],\n",
       "       [  17,   22,   23,   15,   25,   28,    2,   15,   12, 1645]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various techniques have been tried to improve the accuracy of the CNN model:\n",
    "- Adding more hidden layers\n",
    "- Adding more neurons\n",
    "- Adding Dropout layer\n",
    "- Adding/Removing MaxPooling\n",
    "- Changing Optimizer adam, sgd\n",
    "- Changing batchsize\n",
    "- Changing epoch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By employing various techniques of CNN model improvement, the best accuracy I got is around 91%.\n",
    "\n",
    "### The model fine tuning is an exploratory exercise and we may find better parameters by experimenting more."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Updated_R7_ExternalLab_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
