{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84Q8JfvaeZZ6"
   },
   "source": [
    "## Linear Classifier in TensorFlow \n",
    "Using Low Level API in Eager Execution mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sb7Epo0VOB58"
   },
   "source": [
    "### Load tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 62
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2939,
     "status": "ok",
     "timestamp": 1575365314860,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "fHpCNRv1OB5-",
    "outputId": "752dc305-9e8b-4584-db0c-e772963691ae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3582,
     "status": "ok",
     "timestamp": 1575365315553,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "PlKCTombENFo",
    "outputId": "862c5aba-228e-4bb3-edd6-364378012aa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mjtb-EMcm5K0"
   },
   "outputs": [],
   "source": [
    "#Enable Eager Execution if using tensflow version < 2.0\n",
    "#From tensorflow v2.0 onwards, Eager Execution will be enabled by default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxJDmJqqOB6K"
   },
   "source": [
    "### Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3505,
     "status": "ok",
     "timestamp": 1575365315557,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "FhllFLyKOB6N",
    "outputId": "a285ee23-8481-4750-b960-15c65365daad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiObW4V4SIOz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4yQKMiJOB6R"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Data/prices.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgkX6SEqOB6W"
   },
   "source": [
    "### Check all columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4771,
     "status": "ok",
     "timestamp": 1575365316940,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "7K8pWsNQOB6X",
    "outputId": "4db14d29-14b8-4112-ded7-3272b3f3d432"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'date', u'symbol', u'open', u'close', u'low', u'high', u'volume'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4721,
     "status": "ok",
     "timestamp": 1575365316941,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "DCY7M6SiyabS",
    "outputId": "02f01e56-d946-4d61-860d-51e5bf69e5da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 851264 entries, 0 to 851263\n",
      "Data columns (total 7 columns):\n",
      "date      851264 non-null object\n",
      "symbol    851264 non-null object\n",
      "open      851264 non-null float64\n",
      "close     851264 non-null float64\n",
      "low       851264 non-null float64\n",
      "high      851264 non-null float64\n",
      "volume    851264 non-null float64\n",
      "dtypes: float64(5), object(2)\n",
      "memory usage: 45.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dU6X7MpOB6c"
   },
   "source": [
    "### Drop columns `date` and  `symbol`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lh_6spSKOB6e"
   },
   "outputs": [],
   "source": [
    "data.drop(['date','symbol'],axis =1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4594,
     "status": "ok",
     "timestamp": 1575365316947,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "xlwbUgTwOB6i",
    "outputId": "14c183e9-c6ee-47be-cbff-1417a73ad266"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open       close         low        high     volume\n",
       "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
       "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
       "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
       "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
       "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DBv3WWYOB6q"
   },
   "source": [
    "### Consider only first 1000 rows in the dataset for building feature set and target set\n",
    "Target 'Volume' has very high values. Divide 'Volume' by 1000,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_hG9rGBOB6s"
   },
   "outputs": [],
   "source": [
    "data_df = data.iloc[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4490,
     "status": "ok",
     "timestamp": 1575365316950,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "HHiuQpg6zDcl",
    "outputId": "ad5be59b-edd9-43c5-e529-c3c004449366"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4422,
     "status": "ok",
     "timestamp": 1575365316951,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "axjwmt80zOtU",
    "outputId": "d4c5e6ad-89fe-46ac-d61f-5fbee068c9cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data_df['volume'] = data_df['volume'] / 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4399,
     "status": "ok",
     "timestamp": 1575365316953,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "cTdEhXXTztL9",
    "outputId": "b00c573f-1ced-40fa-a87f-92141e125903"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2.1636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2.3864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2.4895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2.0063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1.4086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open       close         low        high  volume\n",
       "0  123.430000  125.839996  122.309998  126.250000  2.1636\n",
       "1  125.239998  119.980003  119.940002  125.540001  2.3864\n",
       "2  116.379997  114.949997  114.930000  119.739998  2.4895\n",
       "3  115.480003  116.620003  113.500000  117.440002  2.0063\n",
       "4  117.010002  114.970001  114.089996  117.330002  1.4086"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3UaApqYOB6x"
   },
   "source": [
    "### Divide the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vA9gkeTd0E7E"
   },
   "outputs": [],
   "source": [
    "X = data_df.drop(['volume'], axis = 1)\n",
    "y = data_df['volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4368,
     "status": "ok",
     "timestamp": 1575365316958,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "YmgbHnsP0dLg",
    "outputId": "6d6d9466-df77-4130-8fcd-4419dea29baf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 4), (1000,))"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LE4U8lTdQJq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DqW2VJcSz2Am"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split (X, y, test_size = 0.3, random_state = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYK-aUuLbrz2"
   },
   "source": [
    "#### Convert Training and Test Data to numpy float32 arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ao-S0tQGcncz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train =np.array(X_train).astype('float32')\n",
    "X_test = np.array(X_test).astype('float32')\n",
    "y_train =np.array(y_train).astype('float32')\n",
    "y_test = np.array(y_test).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5vp9ttNH7e_P"
   },
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5278,
     "status": "ok",
     "timestamp": 1575365318004,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "8xDIsZDD7g9U",
    "outputId": "9eec2976-e6ca-433f-a0e5-d5cb0f5fc314"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "im1ZegbDdKgv"
   },
   "source": [
    "### Normalize the data\n",
    "You can use Normalizer from sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2EkKAy7fOB6y"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "transformer = Normalizer()\n",
    "X_train = transformer.fit_transform(X_train)\n",
    "X_test = transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6vE4eYCOB62"
   },
   "source": [
    "## Building the Model in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "297_qja4OB7A"
   },
   "source": [
    "1.Define Weights and Bias, use tf.zeros to initialize weights and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L205qPeQOB7B"
   },
   "outputs": [],
   "source": [
    "#We are initializing weights and Bias of (Input-Hidden Layer)\n",
    "w1 = tf.random_normal(shape=(4,1))\n",
    "b1 = tf.zeros(shape=(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HgtWA-UIOB7F"
   },
   "source": [
    "2.Define a function to calculate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JveGlx25OB7H"
   },
   "outputs": [],
   "source": [
    "def prediction (x, w, b):\n",
    "  xw_matmul = tf.matmul(x, w)\n",
    "  net = tf.add(xw_matmul, b)\n",
    "  predict = tf.sigmoid(net)\n",
    "  return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TL1hIwf_OB7M"
   },
   "source": [
    "3.Loss (Cost) Function [Mean square error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8VSWPiGXOB7P"
   },
   "outputs": [],
   "source": [
    "def loss(predicted_y, desired_y):\n",
    "  return tf.reduce_mean(tf.square(predicted_y - desired_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzG85FUlOB7U"
   },
   "source": [
    "4.Function to train the Model\n",
    "\n",
    "1.   Record all the mathematical steps to calculate Loss\n",
    "2.   Calculate Gradients of Loss w.r.t weights and bias\n",
    "3.   Update Weights and Bias based on gradients and learning rate to minimize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cj802w-3OB7X"
   },
   "outputs": [],
   "source": [
    "def train(X_train, y_train, w, b, learning_rate=0.01):\n",
    "    \n",
    "    #Record mathematical operations on 'tape' to calculate loss\n",
    "    with tf.GradientTape() as tape:\n",
    "      tape.watch([w, b])\n",
    "      \n",
    "      current_prediction = prediction(X_train, w, b)\n",
    "      current_loss = loss(current_prediction, y_train)\n",
    "    \n",
    "    #Calculate Gradients for Loss with respect to Weights and Bias\n",
    "    dw, db = tape.gradient(current_loss, [w, b])\n",
    "    \n",
    "    #Update Weights at output layer\n",
    "    w = w - learning_rate * dw\n",
    "    b = b - learning_rate * db\n",
    "    \n",
    "    return w, b, current_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xSypb_u8OB7e"
   },
   "source": [
    "## Train the model for 100 epochs \n",
    "1. Observe the training loss at every iteration\n",
    "2. Observe Train loss at every 5th iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5145,
     "status": "ok",
     "timestamp": 1575365318015,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "DVvgj7eQOB7f",
    "outputId": "050d3b23-49d5-44d9-c2f3-5dfb44bd6b73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at epoch 0: 252.179\n",
      "Training loss at epoch 1: 252.076\n",
      "Training loss at epoch 2: 251.975\n",
      "Training loss at epoch 3: 251.875\n",
      "Training loss at epoch 4: 251.777\n",
      "Training loss at epoch 5: 251.681\n",
      "Training loss at epoch 6: 251.588\n",
      "Training loss at epoch 7: 251.496\n",
      "Training loss at epoch 8: 251.407\n",
      "Training loss at epoch 9: 251.319\n",
      "Training loss at epoch 10: 251.235\n",
      "Training loss at epoch 11: 251.152\n",
      "Training loss at epoch 12: 251.072\n",
      "Training loss at epoch 13: 250.995\n",
      "Training loss at epoch 14: 250.919\n",
      "Training loss at epoch 15: 250.846\n",
      "Training loss at epoch 16: 250.776\n",
      "Training loss at epoch 17: 250.708\n",
      "Training loss at epoch 18: 250.642\n",
      "Training loss at epoch 19: 250.578\n",
      "Training loss at epoch 20: 250.517\n",
      "Training loss at epoch 21: 250.457\n",
      "Training loss at epoch 22: 250.400\n",
      "Training loss at epoch 23: 250.345\n",
      "Training loss at epoch 24: 250.292\n",
      "Training loss at epoch 25: 250.240\n",
      "Training loss at epoch 26: 250.190\n",
      "Training loss at epoch 27: 250.143\n",
      "Training loss at epoch 28: 250.096\n",
      "Training loss at epoch 29: 250.052\n",
      "Training loss at epoch 30: 250.009\n",
      "Training loss at epoch 31: 249.968\n",
      "Training loss at epoch 32: 249.928\n",
      "Training loss at epoch 33: 249.889\n",
      "Training loss at epoch 34: 249.852\n",
      "Training loss at epoch 35: 249.816\n",
      "Training loss at epoch 36: 249.781\n",
      "Training loss at epoch 37: 249.748\n",
      "Training loss at epoch 38: 249.715\n",
      "Training loss at epoch 39: 249.684\n",
      "Training loss at epoch 40: 249.654\n",
      "Training loss at epoch 41: 249.625\n",
      "Training loss at epoch 42: 249.596\n",
      "Training loss at epoch 43: 249.569\n",
      "Training loss at epoch 44: 249.543\n",
      "Training loss at epoch 45: 249.517\n",
      "Training loss at epoch 46: 249.492\n",
      "Training loss at epoch 47: 249.468\n",
      "Training loss at epoch 48: 249.445\n",
      "Training loss at epoch 49: 249.423\n",
      "Training loss at epoch 50: 249.401\n",
      "Training loss at epoch 51: 249.380\n",
      "Training loss at epoch 52: 249.359\n",
      "Training loss at epoch 53: 249.340\n",
      "Training loss at epoch 54: 249.320\n",
      "Training loss at epoch 55: 249.302\n",
      "Training loss at epoch 56: 249.284\n",
      "Training loss at epoch 57: 249.266\n",
      "Training loss at epoch 58: 249.249\n",
      "Training loss at epoch 59: 249.232\n",
      "Training loss at epoch 60: 249.216\n",
      "Training loss at epoch 61: 249.200\n",
      "Training loss at epoch 62: 249.185\n",
      "Training loss at epoch 63: 249.170\n",
      "Training loss at epoch 64: 249.156\n",
      "Training loss at epoch 65: 249.142\n",
      "Training loss at epoch 66: 249.128\n",
      "Training loss at epoch 67: 249.115\n",
      "Training loss at epoch 68: 249.102\n",
      "Training loss at epoch 69: 249.089\n",
      "Training loss at epoch 70: 249.077\n",
      "Training loss at epoch 71: 249.065\n",
      "Training loss at epoch 72: 249.053\n",
      "Training loss at epoch 73: 249.041\n",
      "Training loss at epoch 74: 249.030\n",
      "Training loss at epoch 75: 249.019\n",
      "Training loss at epoch 76: 249.009\n",
      "Training loss at epoch 77: 248.998\n",
      "Training loss at epoch 78: 248.988\n",
      "Training loss at epoch 79: 248.978\n",
      "Training loss at epoch 80: 248.969\n",
      "Training loss at epoch 81: 248.959\n",
      "Training loss at epoch 82: 248.950\n",
      "Training loss at epoch 83: 248.941\n",
      "Training loss at epoch 84: 248.932\n",
      "Training loss at epoch 85: 248.924\n",
      "Training loss at epoch 86: 248.915\n",
      "Training loss at epoch 87: 248.907\n",
      "Training loss at epoch 88: 248.899\n",
      "Training loss at epoch 89: 248.891\n",
      "Training loss at epoch 90: 248.883\n",
      "Training loss at epoch 91: 248.876\n",
      "Training loss at epoch 92: 248.869\n",
      "Training loss at epoch 93: 248.861\n",
      "Training loss at epoch 94: 248.854\n",
      "Training loss at epoch 95: 248.847\n",
      "Training loss at epoch 96: 248.840\n",
      "Training loss at epoch 97: 248.834\n",
      "Training loss at epoch 98: 248.827\n",
      "Training loss at epoch 99: 248.821\n"
     ]
    }
   ],
   "source": [
    "for ii in range(100):\n",
    "    w1, b1, current_loss = train(X_train, y_train, w1, b1)\n",
    "    print(\"Training loss at epoch {:d}: {:.3f}\".format(ii, current_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5116,
     "status": "ok",
     "timestamp": 1575365318017,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "DkMY3d5fBzSR",
    "outputId": "33f865c4-b67e-4e76-922b-df9b195536b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at epoch 0: 248.815\n",
      "Training loss at epoch 5: 248.808\n",
      "Training loss at epoch 10: 248.802\n",
      "Training loss at epoch 15: 248.796\n",
      "Training loss at epoch 20: 248.791\n",
      "Training loss at epoch 25: 248.785\n",
      "Training loss at epoch 30: 248.779\n",
      "Training loss at epoch 35: 248.774\n",
      "Training loss at epoch 40: 248.768\n",
      "Training loss at epoch 45: 248.763\n",
      "Training loss at epoch 50: 248.758\n",
      "Training loss at epoch 55: 248.753\n",
      "Training loss at epoch 60: 248.748\n",
      "Training loss at epoch 65: 248.743\n",
      "Training loss at epoch 70: 248.738\n",
      "Training loss at epoch 75: 248.733\n",
      "Training loss at epoch 80: 248.729\n",
      "Training loss at epoch 85: 248.724\n",
      "Training loss at epoch 90: 248.720\n",
      "Training loss at epoch 95: 248.715\n"
     ]
    }
   ],
   "source": [
    "for ii in range(0, 100, 5):\n",
    "    w1, b1, current_loss = train(X_train, y_train, w1, b1)\n",
    "    print(\"Training loss at epoch {:d}: {:.3f}\".format(ii, current_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DOL2ncA1OB7q"
   },
   "source": [
    "### Get the shapes and values of W and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5043,
     "status": "ok",
     "timestamp": 1575365318018,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "ZGvtyTeuOB7r",
    "outputId": "244400ad-b870-4174-d686-494281436833"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=4206, shape=(4, 1), dtype=float32, numpy=\n",
       " array([[0.95462024],\n",
       "        [1.13171   ],\n",
       "        [0.20968984],\n",
       "        [0.6468802 ]], dtype=float32)>,\n",
       " <tf.Tensor: id=4209, shape=(1,), dtype=float32, numpy=array([1.2941393], dtype=float32)>)"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ERq9GOKKciho"
   },
   "source": [
    "### Model Prediction on 1st Examples in Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5017,
     "status": "ok",
     "timestamp": 1575365318019,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "gKGvUWahcihp",
    "outputId": "a3989ffe-cc65-436c-fc91-d2d4090fdd0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=4215, shape=(300, 1), dtype=float32, numpy=\n",
       "array([[940879.44],\n",
       "       [941115.1 ],\n",
       "       [940803.5 ],\n",
       "       [940981.5 ],\n",
       "       [940780.  ],\n",
       "       [940805.  ],\n",
       "       [941011.1 ],\n",
       "       [941189.5 ],\n",
       "       [940917.75],\n",
       "       [940856.56],\n",
       "       [940820.7 ],\n",
       "       [940841.8 ],\n",
       "       [940840.1 ],\n",
       "       [941008.3 ],\n",
       "       [940996.8 ],\n",
       "       [940915.1 ],\n",
       "       [941036.1 ],\n",
       "       [940798.9 ],\n",
       "       [941273.2 ],\n",
       "       [940794.1 ],\n",
       "       [941019.6 ],\n",
       "       [940827.06],\n",
       "       [940910.7 ],\n",
       "       [941040.9 ],\n",
       "       [940912.94],\n",
       "       [940897.94],\n",
       "       [941097.6 ],\n",
       "       [941046.4 ],\n",
       "       [940900.8 ],\n",
       "       [940959.6 ],\n",
       "       [941065.9 ],\n",
       "       [940904.1 ],\n",
       "       [940834.44],\n",
       "       [940820.1 ],\n",
       "       [940907.6 ],\n",
       "       [940878.1 ],\n",
       "       [940948.94],\n",
       "       [941012.2 ],\n",
       "       [940949.7 ],\n",
       "       [940987.44],\n",
       "       [940895.2 ],\n",
       "       [940970.9 ],\n",
       "       [940899.4 ],\n",
       "       [940880.6 ],\n",
       "       [940969.7 ],\n",
       "       [940817.  ],\n",
       "       [940904.75],\n",
       "       [940951.  ],\n",
       "       [940968.1 ],\n",
       "       [941019.3 ],\n",
       "       [940943.  ],\n",
       "       [940851.  ],\n",
       "       [941010.25],\n",
       "       [941364.6 ],\n",
       "       [941005.4 ],\n",
       "       [940961.1 ],\n",
       "       [940897.7 ],\n",
       "       [940852.9 ],\n",
       "       [940851.6 ],\n",
       "       [941168.9 ],\n",
       "       [940863.4 ],\n",
       "       [940924.2 ],\n",
       "       [941032.94],\n",
       "       [940836.8 ],\n",
       "       [941044.3 ],\n",
       "       [940887.06],\n",
       "       [940833.94],\n",
       "       [940849.06],\n",
       "       [940859.3 ],\n",
       "       [940931.8 ],\n",
       "       [941178.8 ],\n",
       "       [940935.  ],\n",
       "       [941009.8 ],\n",
       "       [940920.4 ],\n",
       "       [941237.44],\n",
       "       [941385.44],\n",
       "       [941175.5 ],\n",
       "       [940962.2 ],\n",
       "       [940996.06],\n",
       "       [940873.2 ],\n",
       "       [941002.4 ],\n",
       "       [940872.94],\n",
       "       [940830.3 ],\n",
       "       [940971.75],\n",
       "       [941016.6 ],\n",
       "       [940920.94],\n",
       "       [940844.  ],\n",
       "       [940989.  ],\n",
       "       [940948.4 ],\n",
       "       [940890.06],\n",
       "       [941017.5 ],\n",
       "       [940856.3 ],\n",
       "       [940938.  ],\n",
       "       [941005.7 ],\n",
       "       [940897.9 ],\n",
       "       [941078.8 ],\n",
       "       [940812.2 ],\n",
       "       [940937.6 ],\n",
       "       [941092.7 ],\n",
       "       [940835.8 ],\n",
       "       [940898.1 ],\n",
       "       [940841.4 ],\n",
       "       [940901.25],\n",
       "       [940994.25],\n",
       "       [941034.44],\n",
       "       [940937.5 ],\n",
       "       [940920.7 ],\n",
       "       [940908.3 ],\n",
       "       [941006.06],\n",
       "       [941084.25],\n",
       "       [940959.7 ],\n",
       "       [940843.1 ],\n",
       "       [940874.9 ],\n",
       "       [940999.4 ],\n",
       "       [941231.56],\n",
       "       [941026.56],\n",
       "       [940991.06],\n",
       "       [940979.7 ],\n",
       "       [940915.94],\n",
       "       [941073.06],\n",
       "       [940852.56],\n",
       "       [940995.7 ],\n",
       "       [940979.4 ],\n",
       "       [941003.44],\n",
       "       [940921.6 ],\n",
       "       [940843.94],\n",
       "       [940900.25],\n",
       "       [940993.9 ],\n",
       "       [940877.7 ],\n",
       "       [940861.56],\n",
       "       [940882.1 ],\n",
       "       [940883.5 ],\n",
       "       [940880.6 ],\n",
       "       [940809.75],\n",
       "       [940929.9 ],\n",
       "       [940790.3 ],\n",
       "       [940837.1 ],\n",
       "       [941001.1 ],\n",
       "       [940911.06],\n",
       "       [940921.9 ],\n",
       "       [940868.  ],\n",
       "       [940965.75],\n",
       "       [940829.5 ],\n",
       "       [940924.06],\n",
       "       [940804.6 ],\n",
       "       [940907.25],\n",
       "       [940894.75],\n",
       "       [940861.25],\n",
       "       [941088.94],\n",
       "       [940848.  ],\n",
       "       [940886.75],\n",
       "       [940846.2 ],\n",
       "       [940942.5 ],\n",
       "       [940923.5 ],\n",
       "       [940963.94],\n",
       "       [940966.75],\n",
       "       [940898.56],\n",
       "       [940967.44],\n",
       "       [940922.4 ],\n",
       "       [940839.06],\n",
       "       [940954.06],\n",
       "       [940886.1 ],\n",
       "       [940988.75],\n",
       "       [940866.5 ],\n",
       "       [940825.94],\n",
       "       [940912.5 ],\n",
       "       [941430.8 ],\n",
       "       [941072.1 ],\n",
       "       [941102.5 ],\n",
       "       [941005.2 ],\n",
       "       [940832.25],\n",
       "       [940942.06],\n",
       "       [940889.5 ],\n",
       "       [940863.8 ],\n",
       "       [940859.25],\n",
       "       [940884.2 ],\n",
       "       [940907.8 ],\n",
       "       [940938.8 ],\n",
       "       [941001.56],\n",
       "       [940867.5 ],\n",
       "       [941230.6 ],\n",
       "       [940998.44],\n",
       "       [940824.2 ],\n",
       "       [941071.75],\n",
       "       [940862.75],\n",
       "       [941011.2 ],\n",
       "       [940822.9 ],\n",
       "       [940915.  ],\n",
       "       [940944.06],\n",
       "       [941023.3 ],\n",
       "       [940806.4 ],\n",
       "       [940821.2 ],\n",
       "       [941004.9 ],\n",
       "       [940873.75],\n",
       "       [941026.7 ],\n",
       "       [940967.7 ],\n",
       "       [940911.44],\n",
       "       [940942.2 ],\n",
       "       [940919.6 ],\n",
       "       [941029.7 ],\n",
       "       [940943.1 ],\n",
       "       [940960.56],\n",
       "       [940911.75],\n",
       "       [940823.4 ],\n",
       "       [940976.4 ],\n",
       "       [940847.75],\n",
       "       [940974.94],\n",
       "       [940838.  ],\n",
       "       [940837.94],\n",
       "       [940879.4 ],\n",
       "       [941257.5 ],\n",
       "       [941088.06],\n",
       "       [940805.44],\n",
       "       [940858.8 ],\n",
       "       [940872.94],\n",
       "       [940840.6 ],\n",
       "       [941034.06],\n",
       "       [940896.2 ],\n",
       "       [940856.4 ],\n",
       "       [940860.56],\n",
       "       [940938.3 ],\n",
       "       [941189.9 ],\n",
       "       [940967.9 ],\n",
       "       [940861.25],\n",
       "       [940907.8 ],\n",
       "       [940904.4 ],\n",
       "       [940964.7 ],\n",
       "       [941113.1 ],\n",
       "       [940884.25],\n",
       "       [940963.4 ],\n",
       "       [941134.8 ],\n",
       "       [941018.94],\n",
       "       [940961.25],\n",
       "       [940868.9 ],\n",
       "       [940908.3 ],\n",
       "       [941019.75],\n",
       "       [940852.8 ],\n",
       "       [940917.7 ],\n",
       "       [940851.44],\n",
       "       [940908.94],\n",
       "       [940978.2 ],\n",
       "       [940843.8 ],\n",
       "       [940819.4 ],\n",
       "       [940937.6 ],\n",
       "       [941264.7 ],\n",
       "       [941156.4 ],\n",
       "       [940853.5 ],\n",
       "       [940913.44],\n",
       "       [941251.9 ],\n",
       "       [940870.4 ],\n",
       "       [940994.44],\n",
       "       [940843.1 ],\n",
       "       [940810.06],\n",
       "       [940929.2 ],\n",
       "       [940902.1 ],\n",
       "       [941123.2 ],\n",
       "       [940957.56],\n",
       "       [940833.8 ],\n",
       "       [940871.3 ],\n",
       "       [941073.56],\n",
       "       [941018.5 ],\n",
       "       [941273.25],\n",
       "       [940945.4 ],\n",
       "       [940997.25],\n",
       "       [941027.4 ],\n",
       "       [940897.4 ],\n",
       "       [940855.8 ],\n",
       "       [940886.4 ],\n",
       "       [940761.25],\n",
       "       [940912.6 ],\n",
       "       [940945.25],\n",
       "       [940990.06],\n",
       "       [940937.25],\n",
       "       [940816.1 ],\n",
       "       [940909.5 ],\n",
       "       [940848.9 ],\n",
       "       [940839.44],\n",
       "       [940863.94],\n",
       "       [940962.56],\n",
       "       [940948.56],\n",
       "       [940947.9 ],\n",
       "       [940992.7 ],\n",
       "       [940854.2 ],\n",
       "       [941026.7 ],\n",
       "       [940795.9 ],\n",
       "       [940896.2 ],\n",
       "       [940995.7 ],\n",
       "       [940971.75],\n",
       "       [940803.9 ],\n",
       "       [940927.25],\n",
       "       [940956.1 ],\n",
       "       [941020.44],\n",
       "       [940928.94],\n",
       "       [940855.75],\n",
       "       [940888.6 ],\n",
       "       [940830.94],\n",
       "       [940839.6 ],\n",
       "       [940955.8 ],\n",
       "       [940818.44],\n",
       "       [940958.7 ]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = prediction(X_test, w1, b1)\n",
    "y_pred * 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJRBuqXhOB7_"
   },
   "source": [
    "## Classification using tf.Keras\n",
    "\n",
    "In this exercise, we will build a Deep Neural Network using tf.Keras. We will use Iris Dataset for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0g6lorycihf"
   },
   "source": [
    "### Load the given Iris data using pandas (Iris.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6xFvb5sRcihg"
   },
   "outputs": [],
   "source": [
    "iris = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/Data/11_Iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SAB--Qdwcihm"
   },
   "source": [
    "### Target set has different categories. So, Label encode them. And convert into one-hot vectors using get_dummies in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4962,
     "status": "ok",
     "timestamp": 1575365318021,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "IJr5dYnocihm",
    "outputId": "da7f6514-7c36-4087-d695-02ba17abd308"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hMecdmpGLey1"
   },
   "outputs": [],
   "source": [
    "dummy = pd.get_dummies(iris['Species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AhJASNFqRLVy"
   },
   "outputs": [],
   "source": [
    "iris = pd.concat([iris, dummy], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gvfjgiZpSfjy"
   },
   "outputs": [],
   "source": [
    "iris.drop(labels=['Id', 'Species'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4850,
     "status": "ok",
     "timestamp": 1575365318027,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "bWBNA6TKRsTY",
    "outputId": "0e4a6673-cf78-4bdc-ab0e-953d2181bc88"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Iris-setosa</th>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <th>Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLengthCm  SepalWidthCm  ...  Iris-versicolor  Iris-virginica\n",
       "0            5.1           3.5  ...                0               0\n",
       "1            4.9           3.0  ...                0               0\n",
       "2            4.7           3.2  ...                0               0\n",
       "3            4.6           3.1  ...                0               0\n",
       "4            5.0           3.6  ...                0               0\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D95nY5ILcihj"
   },
   "source": [
    "### Splitting the data into feature set and target set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWTxF7Dgcx6d"
   },
   "outputs": [],
   "source": [
    "X = iris.drop(labels=['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], axis=1)\n",
    "y = iris.drop(labels=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qbV7vcZnkQr7"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tGa0pzNpgEo8"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YQe9e1csgH45"
   },
   "outputs": [],
   "source": [
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d4Og34dleo-r"
   },
   "outputs": [],
   "source": [
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4635,
     "status": "ok",
     "timestamp": 1575365318033,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "gvmezB-mSMeT",
    "outputId": "343d07e7-569c-472f-ca55-09515c2b5a24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X), type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b22qpC5xcihr"
   },
   "source": [
    "###  Building Model in tf.keras\n",
    "\n",
    "Build a Linear Classifier model  <br>\n",
    "1.  Use Dense Layer  with input shape of 4 (according to the feature set) and number of outputs set to 3<br> \n",
    "2. Apply Softmax on Dense Layer outputs <br>\n",
    "3. Use SGD as Optimizer\n",
    "4. Use categorical_crossentropy as loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LoKsD1FeXXm-"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split (X, y, test_size = 0.3, random_state = 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hov_UFnUciht"
   },
   "outputs": [],
   "source": [
    "# define base model\n",
    "model = tf.keras.models.Sequential()\n",
    "# Normalize the data\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "# Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(3, input_dim=4, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile( optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T5FdzqIKcihw"
   },
   "source": [
    "### Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8931,
     "status": "ok",
     "timestamp": 1575365322419,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "4qLEdHPscihx",
    "outputId": "5a834773-a9f2-4712-ff04-2d22e879f77c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1203 09:28:38.293096 140665610463104 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:12: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 1s 11ms/sample - loss: 0.6898 - acc: 0.6095 - val_loss: 0.7350 - val_acc: 0.6222\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - 0s 137us/sample - loss: 0.6860 - acc: 0.6095 - val_loss: 0.7313 - val_acc: 0.6222\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - 0s 125us/sample - loss: 0.6822 - acc: 0.6095 - val_loss: 0.7277 - val_acc: 0.6222\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - 0s 116us/sample - loss: 0.6785 - acc: 0.6095 - val_loss: 0.7241 - val_acc: 0.6222\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - 0s 128us/sample - loss: 0.6748 - acc: 0.6095 - val_loss: 0.7206 - val_acc: 0.6222\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - 0s 122us/sample - loss: 0.6712 - acc: 0.6095 - val_loss: 0.7172 - val_acc: 0.6444\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - 0s 100us/sample - loss: 0.6677 - acc: 0.6095 - val_loss: 0.7137 - val_acc: 0.6444\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - 0s 109us/sample - loss: 0.6641 - acc: 0.6095 - val_loss: 0.7104 - val_acc: 0.6444\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - 0s 124us/sample - loss: 0.6607 - acc: 0.6095 - val_loss: 0.7071 - val_acc: 0.6444\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - 0s 111us/sample - loss: 0.6573 - acc: 0.6095 - val_loss: 0.7038 - val_acc: 0.6444\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - 0s 116us/sample - loss: 0.6540 - acc: 0.6190 - val_loss: 0.7006 - val_acc: 0.6444\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - 0s 125us/sample - loss: 0.6507 - acc: 0.6190 - val_loss: 0.6974 - val_acc: 0.6444\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - 0s 102us/sample - loss: 0.6474 - acc: 0.6190 - val_loss: 0.6943 - val_acc: 0.6444\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - 0s 109us/sample - loss: 0.6442 - acc: 0.6190 - val_loss: 0.6912 - val_acc: 0.6444\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - 0s 93us/sample - loss: 0.6410 - acc: 0.6286 - val_loss: 0.6882 - val_acc: 0.6444\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - 0s 134us/sample - loss: 0.6379 - acc: 0.6286 - val_loss: 0.6852 - val_acc: 0.6222\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - 0s 122us/sample - loss: 0.6349 - acc: 0.6286 - val_loss: 0.6822 - val_acc: 0.6222\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - 0s 145us/sample - loss: 0.6318 - acc: 0.6286 - val_loss: 0.6793 - val_acc: 0.6222\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - 0s 135us/sample - loss: 0.6288 - acc: 0.6381 - val_loss: 0.6764 - val_acc: 0.6000\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - 0s 105us/sample - loss: 0.6259 - acc: 0.6381 - val_loss: 0.6736 - val_acc: 0.6000\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - 0s 120us/sample - loss: 0.6230 - acc: 0.6476 - val_loss: 0.6708 - val_acc: 0.6000\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - 0s 105us/sample - loss: 0.6201 - acc: 0.6476 - val_loss: 0.6680 - val_acc: 0.6000\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - 0s 116us/sample - loss: 0.6173 - acc: 0.6571 - val_loss: 0.6653 - val_acc: 0.6222\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - 0s 130us/sample - loss: 0.6145 - acc: 0.6571 - val_loss: 0.6626 - val_acc: 0.6222\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - 0s 337us/sample - loss: 0.6118 - acc: 0.6667 - val_loss: 0.6600 - val_acc: 0.6222\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - 0s 184us/sample - loss: 0.6091 - acc: 0.6857 - val_loss: 0.6574 - val_acc: 0.6222\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - 0s 169us/sample - loss: 0.6064 - acc: 0.6857 - val_loss: 0.6548 - val_acc: 0.6444\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - 0s 130us/sample - loss: 0.6038 - acc: 0.6762 - val_loss: 0.6523 - val_acc: 0.6444\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - 0s 181us/sample - loss: 0.6012 - acc: 0.6762 - val_loss: 0.6498 - val_acc: 0.6444\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - 0s 158us/sample - loss: 0.5987 - acc: 0.6762 - val_loss: 0.6473 - val_acc: 0.6667\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - 0s 153us/sample - loss: 0.5961 - acc: 0.6952 - val_loss: 0.6449 - val_acc: 0.6889\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - 0s 151us/sample - loss: 0.5936 - acc: 0.7048 - val_loss: 0.6425 - val_acc: 0.6889\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - 0s 105us/sample - loss: 0.5912 - acc: 0.7143 - val_loss: 0.6401 - val_acc: 0.7111\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - 0s 155us/sample - loss: 0.5888 - acc: 0.7238 - val_loss: 0.6378 - val_acc: 0.7556\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - 0s 142us/sample - loss: 0.5864 - acc: 0.7143 - val_loss: 0.6355 - val_acc: 0.7778\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.5840 - acc: 0.7048 - val_loss: 0.6332 - val_acc: 0.8000\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - 0s 146us/sample - loss: 0.5817 - acc: 0.7143 - val_loss: 0.6309 - val_acc: 0.8000\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - 0s 138us/sample - loss: 0.5794 - acc: 0.7048 - val_loss: 0.6287 - val_acc: 0.8000\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 0.5771 - acc: 0.7143 - val_loss: 0.6265 - val_acc: 0.8000\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - 0s 138us/sample - loss: 0.5749 - acc: 0.7238 - val_loss: 0.6244 - val_acc: 0.8000\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - 0s 136us/sample - loss: 0.5727 - acc: 0.7238 - val_loss: 0.6223 - val_acc: 0.8000\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - 0s 147us/sample - loss: 0.5705 - acc: 0.7238 - val_loss: 0.6202 - val_acc: 0.8000\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - 0s 130us/sample - loss: 0.5684 - acc: 0.7429 - val_loss: 0.6181 - val_acc: 0.8000\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - 0s 136us/sample - loss: 0.5662 - acc: 0.7619 - val_loss: 0.6160 - val_acc: 0.8222\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - 0s 97us/sample - loss: 0.5642 - acc: 0.7619 - val_loss: 0.6140 - val_acc: 0.8222\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - 0s 125us/sample - loss: 0.5621 - acc: 0.7714 - val_loss: 0.6120 - val_acc: 0.8000\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - 0s 95us/sample - loss: 0.5601 - acc: 0.7714 - val_loss: 0.6101 - val_acc: 0.8000\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - 0s 103us/sample - loss: 0.5580 - acc: 0.7714 - val_loss: 0.6081 - val_acc: 0.8444\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - 0s 160us/sample - loss: 0.5561 - acc: 0.7714 - val_loss: 0.6062 - val_acc: 0.8444\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - 0s 143us/sample - loss: 0.5541 - acc: 0.7714 - val_loss: 0.6043 - val_acc: 0.8444\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.5522 - acc: 0.7714 - val_loss: 0.6024 - val_acc: 0.8444\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - 0s 148us/sample - loss: 0.5503 - acc: 0.7714 - val_loss: 0.6006 - val_acc: 0.8222\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - 0s 115us/sample - loss: 0.5484 - acc: 0.7810 - val_loss: 0.5988 - val_acc: 0.8222\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - 0s 161us/sample - loss: 0.5465 - acc: 0.8000 - val_loss: 0.5970 - val_acc: 0.8222\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - 0s 154us/sample - loss: 0.5447 - acc: 0.8095 - val_loss: 0.5952 - val_acc: 0.8000\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - 0s 138us/sample - loss: 0.5428 - acc: 0.8095 - val_loss: 0.5935 - val_acc: 0.7778\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - 0s 135us/sample - loss: 0.5411 - acc: 0.8095 - val_loss: 0.5917 - val_acc: 0.7778\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - 0s 106us/sample - loss: 0.5393 - acc: 0.8000 - val_loss: 0.5900 - val_acc: 0.7778\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - 0s 120us/sample - loss: 0.5375 - acc: 0.8000 - val_loss: 0.5883 - val_acc: 0.7778\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - 0s 131us/sample - loss: 0.5358 - acc: 0.8000 - val_loss: 0.5867 - val_acc: 0.7778\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.5341 - acc: 0.8000 - val_loss: 0.5850 - val_acc: 0.7778\n",
      "Epoch 62/100\n",
      "105/105 [==============================] - 0s 109us/sample - loss: 0.5324 - acc: 0.8000 - val_loss: 0.5834 - val_acc: 0.7778\n",
      "Epoch 63/100\n",
      "105/105 [==============================] - 0s 168us/sample - loss: 0.5308 - acc: 0.8000 - val_loss: 0.5818 - val_acc: 0.7778\n",
      "Epoch 64/100\n",
      "105/105 [==============================] - 0s 144us/sample - loss: 0.5291 - acc: 0.7905 - val_loss: 0.5802 - val_acc: 0.7778\n",
      "Epoch 65/100\n",
      "105/105 [==============================] - 0s 184us/sample - loss: 0.5275 - acc: 0.7905 - val_loss: 0.5786 - val_acc: 0.8000\n",
      "Epoch 66/100\n",
      "105/105 [==============================] - 0s 94us/sample - loss: 0.5259 - acc: 0.7905 - val_loss: 0.5771 - val_acc: 0.8000\n",
      "Epoch 67/100\n",
      "105/105 [==============================] - 0s 163us/sample - loss: 0.5243 - acc: 0.7905 - val_loss: 0.5756 - val_acc: 0.8000\n",
      "Epoch 68/100\n",
      "105/105 [==============================] - 0s 135us/sample - loss: 0.5227 - acc: 0.7905 - val_loss: 0.5741 - val_acc: 0.8000\n",
      "Epoch 69/100\n",
      "105/105 [==============================] - 0s 129us/sample - loss: 0.5212 - acc: 0.7810 - val_loss: 0.5726 - val_acc: 0.8000\n",
      "Epoch 70/100\n",
      "105/105 [==============================] - 0s 144us/sample - loss: 0.5197 - acc: 0.7810 - val_loss: 0.5711 - val_acc: 0.8000\n",
      "Epoch 71/100\n",
      "105/105 [==============================] - 0s 138us/sample - loss: 0.5181 - acc: 0.7905 - val_loss: 0.5696 - val_acc: 0.8000\n",
      "Epoch 72/100\n",
      "105/105 [==============================] - 0s 179us/sample - loss: 0.5167 - acc: 0.7905 - val_loss: 0.5682 - val_acc: 0.8000\n",
      "Epoch 73/100\n",
      "105/105 [==============================] - 0s 108us/sample - loss: 0.5152 - acc: 0.7905 - val_loss: 0.5668 - val_acc: 0.8000\n",
      "Epoch 74/100\n",
      "105/105 [==============================] - 0s 166us/sample - loss: 0.5137 - acc: 0.8000 - val_loss: 0.5654 - val_acc: 0.8000\n",
      "Epoch 75/100\n",
      "105/105 [==============================] - 0s 130us/sample - loss: 0.5123 - acc: 0.8000 - val_loss: 0.5640 - val_acc: 0.8000\n",
      "Epoch 76/100\n",
      "105/105 [==============================] - 0s 132us/sample - loss: 0.5109 - acc: 0.8000 - val_loss: 0.5626 - val_acc: 0.8000\n",
      "Epoch 77/100\n",
      "105/105 [==============================] - 0s 134us/sample - loss: 0.5095 - acc: 0.8000 - val_loss: 0.5613 - val_acc: 0.8000\n",
      "Epoch 78/100\n",
      "105/105 [==============================] - 0s 120us/sample - loss: 0.5081 - acc: 0.8000 - val_loss: 0.5599 - val_acc: 0.8000\n",
      "Epoch 79/100\n",
      "105/105 [==============================] - 0s 132us/sample - loss: 0.5067 - acc: 0.8000 - val_loss: 0.5586 - val_acc: 0.8000\n",
      "Epoch 80/100\n",
      "105/105 [==============================] - 0s 135us/sample - loss: 0.5053 - acc: 0.8000 - val_loss: 0.5573 - val_acc: 0.8000\n",
      "Epoch 81/100\n",
      "105/105 [==============================] - 0s 174us/sample - loss: 0.5040 - acc: 0.8000 - val_loss: 0.5560 - val_acc: 0.8000\n",
      "Epoch 82/100\n",
      "105/105 [==============================] - 0s 153us/sample - loss: 0.5027 - acc: 0.8095 - val_loss: 0.5547 - val_acc: 0.8000\n",
      "Epoch 83/100\n",
      "105/105 [==============================] - 0s 140us/sample - loss: 0.5013 - acc: 0.8190 - val_loss: 0.5535 - val_acc: 0.8000\n",
      "Epoch 84/100\n",
      "105/105 [==============================] - 0s 126us/sample - loss: 0.5000 - acc: 0.8190 - val_loss: 0.5522 - val_acc: 0.8000\n",
      "Epoch 85/100\n",
      "105/105 [==============================] - 0s 205us/sample - loss: 0.4988 - acc: 0.8190 - val_loss: 0.5510 - val_acc: 0.8000\n",
      "Epoch 86/100\n",
      "105/105 [==============================] - 0s 152us/sample - loss: 0.4975 - acc: 0.8095 - val_loss: 0.5498 - val_acc: 0.8000\n",
      "Epoch 87/100\n",
      "105/105 [==============================] - 0s 139us/sample - loss: 0.4962 - acc: 0.8095 - val_loss: 0.5485 - val_acc: 0.8000\n",
      "Epoch 88/100\n",
      "105/105 [==============================] - 0s 135us/sample - loss: 0.4950 - acc: 0.8095 - val_loss: 0.5473 - val_acc: 0.8000\n",
      "Epoch 89/100\n",
      "105/105 [==============================] - 0s 143us/sample - loss: 0.4938 - acc: 0.8095 - val_loss: 0.5462 - val_acc: 0.8000\n",
      "Epoch 90/100\n",
      "105/105 [==============================] - 0s 158us/sample - loss: 0.4926 - acc: 0.8095 - val_loss: 0.5450 - val_acc: 0.8000\n",
      "Epoch 91/100\n",
      "105/105 [==============================] - 0s 111us/sample - loss: 0.4914 - acc: 0.8095 - val_loss: 0.5438 - val_acc: 0.8000\n",
      "Epoch 92/100\n",
      "105/105 [==============================] - 0s 143us/sample - loss: 0.4902 - acc: 0.8095 - val_loss: 0.5427 - val_acc: 0.8000\n",
      "Epoch 93/100\n",
      "105/105 [==============================] - 0s 162us/sample - loss: 0.4890 - acc: 0.8095 - val_loss: 0.5416 - val_acc: 0.8000\n",
      "Epoch 94/100\n",
      "105/105 [==============================] - 0s 116us/sample - loss: 0.4878 - acc: 0.8095 - val_loss: 0.5404 - val_acc: 0.8000\n",
      "Epoch 95/100\n",
      "105/105 [==============================] - 0s 124us/sample - loss: 0.4867 - acc: 0.8095 - val_loss: 0.5393 - val_acc: 0.8000\n",
      "Epoch 96/100\n",
      "105/105 [==============================] - 0s 134us/sample - loss: 0.4856 - acc: 0.8095 - val_loss: 0.5382 - val_acc: 0.8000\n",
      "Epoch 97/100\n",
      "105/105 [==============================] - 0s 125us/sample - loss: 0.4844 - acc: 0.8095 - val_loss: 0.5372 - val_acc: 0.8000\n",
      "Epoch 98/100\n",
      "105/105 [==============================] - 0s 124us/sample - loss: 0.4833 - acc: 0.8095 - val_loss: 0.5361 - val_acc: 0.8000\n",
      "Epoch 99/100\n",
      "105/105 [==============================] - 0s 151us/sample - loss: 0.4822 - acc: 0.8095 - val_loss: 0.5350 - val_acc: 0.8000\n",
      "Epoch 100/100\n",
      "105/105 [==============================] - 0s 127us/sample - loss: 0.4811 - acc: 0.8095 - val_loss: 0.5340 - val_acc: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7feedb6bebd0>"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=100, batch_size = X_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y-SgSSdRcih5"
   },
   "source": [
    "### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GBgKZkhkcih6"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8875,
     "status": "ok",
     "timestamp": 1575365322423,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "jpjqeYkefq0W",
    "outputId": "37fe8ee7-5a2c-4d2b-d169-93fca6ff9e5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06689516, 0.33502224, 0.59808254],\n",
       "       [0.71006113, 0.19277047, 0.09716837],\n",
       "       [0.19120799, 0.38621882, 0.4225732 ],\n",
       "       [0.05070156, 0.3112746 , 0.63802385],\n",
       "       [0.9678472 , 0.01818296, 0.01396976],\n",
       "       [0.04424505, 0.5326995 , 0.4230554 ],\n",
       "       [0.0955571 , 0.42313498, 0.48130792],\n",
       "       [0.01325559, 0.44650623, 0.54023814],\n",
       "       [0.90581894, 0.0551342 , 0.03904688],\n",
       "       [0.0984573 , 0.36364603, 0.5378967 ],\n",
       "       [0.02374434, 0.35033193, 0.6259237 ],\n",
       "       [0.03583222, 0.535475  , 0.42869282],\n",
       "       [0.06398857, 0.3947252 , 0.54128623],\n",
       "       [0.06996042, 0.488322  , 0.44171757],\n",
       "       [0.07560674, 0.30904615, 0.61534715],\n",
       "       [0.98738617, 0.00635763, 0.00625621],\n",
       "       [0.87697905, 0.07814807, 0.04487296],\n",
       "       [0.15106714, 0.42792347, 0.4210094 ],\n",
       "       [0.06542039, 0.5593542 , 0.37522545],\n",
       "       [0.01003288, 0.5258347 , 0.46413243],\n",
       "       [0.07818997, 0.43283594, 0.48897403],\n",
       "       [0.25251517, 0.31382632, 0.43365857],\n",
       "       [0.9799861 , 0.01067367, 0.00934033],\n",
       "       [0.05259189, 0.55812216, 0.38928595],\n",
       "       [0.20718294, 0.18923922, 0.60357785],\n",
       "       [0.9128138 , 0.05168781, 0.03549843],\n",
       "       [0.9129618 , 0.04734248, 0.03969573],\n",
       "       [0.05145696, 0.3713055 , 0.57723755],\n",
       "       [0.17921008, 0.3828171 , 0.43797284],\n",
       "       [0.02353641, 0.3566532 , 0.6198104 ],\n",
       "       [0.8387706 , 0.10132413, 0.0599053 ],\n",
       "       [0.05481412, 0.3781707 , 0.5670152 ],\n",
       "       [0.99708986, 0.00124616, 0.00166391],\n",
       "       [0.0310009 , 0.48830327, 0.48069587],\n",
       "       [0.08422355, 0.49896848, 0.416808  ],\n",
       "       [0.8902074 , 0.0685629 , 0.04122965],\n",
       "       [0.92766565, 0.04363635, 0.02869795],\n",
       "       [0.04104126, 0.31274527, 0.6462135 ],\n",
       "       [0.0442275 , 0.35495856, 0.600814  ],\n",
       "       [0.1668106 , 0.4019376 , 0.43125173],\n",
       "       [0.21786556, 0.3294718 , 0.45266262],\n",
       "       [0.13237888, 0.47296336, 0.39465776],\n",
       "       [0.04430631, 0.4270223 , 0.5286714 ],\n",
       "       [0.10534816, 0.3131446 , 0.5815072 ],\n",
       "       [0.11681645, 0.47167298, 0.41151047]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RyD70GbErVY_"
   },
   "source": [
    "### Getting the index of maximum probability for every test row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8851,
     "status": "ok",
     "timestamp": 1575365322425,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "ku6tnXommH2m",
    "outputId": "e8bbbd2b-8025-474a-8dc7-19dcb769e1ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 2, 2, 0, 1, 2, 2, 0, 2, 2, 1, 2, 1, 2, 0, 0, 1, 1, 1, 2, 2,\n",
       "       0, 1, 2, 0, 0, 2, 2, 2, 0, 2, 0, 1, 1, 0, 0, 2, 2, 2, 2, 1, 2, 2,\n",
       "       1])"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_max = np.argmax(y_pred, axis=1)\n",
    "y_pred_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wLc_vr-Urjbb"
   },
   "source": [
    "### Creating a blank array for setting the predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9GNn7pdKmfoa"
   },
   "outputs": [],
   "source": [
    "y_pred_class = np.zeros(shape=y_test.shape, dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5BlMPd3drvC-"
   },
   "source": [
    "### Setting the Class as True for those index with maximum probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W53H1H4BmrN5"
   },
   "outputs": [],
   "source": [
    "for ii in range(y_pred.shape[0]):\n",
    "  y_pred_class[ii, y_pred_max[ii]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lzSJvQF9sGZs"
   },
   "source": [
    "### These are predicted classes with the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8780,
     "status": "ok",
     "timestamp": 1575365322430,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "ZGqCXW9nqDNf",
    "outputId": "0595ddb7-8ac4-4cd2-98d9-a858dbe02c8c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0]], dtype=int32)"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ap1hAY1_5vDw"
   },
   "source": [
    "### Reset the Class labels for the dummy columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ip1Eutw14Py1"
   },
   "outputs": [],
   "source": [
    "dummy_cols = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "\n",
    "classes = pd.DataFrame()\n",
    "\n",
    "for ii in range(y_pred.shape[0]):\n",
    "  classes.loc[ii, 'Species'] = dummy_cols[y_pred_max[ii]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8t-_WA6rBKbn"
   },
   "source": [
    "### These are the predicted Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8730,
     "status": "ok",
     "timestamp": 1575365322432,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "MdBVxPA35jDy",
    "outputId": "26fa792a-1217-4765-beca-c63c315b9bca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Species\n",
       "0    Iris-virginica\n",
       "1       Iris-setosa\n",
       "2    Iris-virginica\n",
       "3    Iris-virginica\n",
       "4       Iris-setosa\n",
       "5   Iris-versicolor\n",
       "6    Iris-virginica\n",
       "7    Iris-virginica\n",
       "8       Iris-setosa\n",
       "9    Iris-virginica\n",
       "10   Iris-virginica\n",
       "11  Iris-versicolor\n",
       "12   Iris-virginica\n",
       "13  Iris-versicolor\n",
       "14   Iris-virginica\n",
       "15      Iris-setosa\n",
       "16      Iris-setosa\n",
       "17  Iris-versicolor\n",
       "18  Iris-versicolor\n",
       "19  Iris-versicolor\n",
       "20   Iris-virginica\n",
       "21   Iris-virginica\n",
       "22      Iris-setosa\n",
       "23  Iris-versicolor\n",
       "24   Iris-virginica\n",
       "25      Iris-setosa\n",
       "26      Iris-setosa\n",
       "27   Iris-virginica\n",
       "28   Iris-virginica\n",
       "29   Iris-virginica\n",
       "30      Iris-setosa\n",
       "31   Iris-virginica\n",
       "32      Iris-setosa\n",
       "33  Iris-versicolor\n",
       "34  Iris-versicolor\n",
       "35      Iris-setosa\n",
       "36      Iris-setosa\n",
       "37   Iris-virginica\n",
       "38   Iris-virginica\n",
       "39   Iris-virginica\n",
       "40   Iris-virginica\n",
       "41  Iris-versicolor\n",
       "42   Iris-virginica\n",
       "43   Iris-virginica\n",
       "44  Iris-versicolor"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P32ASP1Vjt0a"
   },
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n8rd0jjAjyTR"
   },
   "outputs": [],
   "source": [
    "model.save('iris_model-santosh.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XiipRpe7rbVh"
   },
   "source": [
    "### Build and Train a Deep Neural network with 2 hidden layer  - Optional - For Practice\n",
    "\n",
    "Does it perform better than Linear Classifier? What could be the reason for difference in performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v5Du3lubr4sA"
   },
   "outputs": [],
   "source": [
    "# define base model\n",
    "model2 = tf.keras.models.Sequential()\n",
    "# Normalize the data\n",
    "model2.add(tf.keras.layers.BatchNormalization())\n",
    "# Add 2 more hidden layer with relu activation function:\n",
    "model2.add(tf.keras.layers.Dense(4, input_dim=4, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dense(3, input_dim=4, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dense(3, input_dim=3, activation='softmax'))\n",
    "# Compile model\n",
    "model2.compile( optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10361,
     "status": "ok",
     "timestamp": 1575365324125,
     "user": {
      "displayName": "Santosh Kulkarni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDmeIHDJbwTZS-iFO8rlNCs50kBwUt8ms9DGthD=s64",
      "userId": "07011934147846339398"
     },
     "user_tz": -330
    },
    "id": "-2idTMzjyWaB",
    "outputId": "af9e9740-1054-4d1e-90ea-0d84b1510f83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - 0s 4ms/sample - loss: 1.0198 - acc: 0.3429 - val_loss: 1.0360 - val_acc: 0.3778\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - 0s 139us/sample - loss: 1.0168 - acc: 0.3524 - val_loss: 1.0328 - val_acc: 0.3778\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - 0s 107us/sample - loss: 1.0139 - acc: 0.3524 - val_loss: 1.0296 - val_acc: 0.3778\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - 0s 119us/sample - loss: 1.0109 - acc: 0.3619 - val_loss: 1.0264 - val_acc: 0.3778\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - 0s 115us/sample - loss: 1.0080 - acc: 0.3619 - val_loss: 1.0233 - val_acc: 0.3778\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - 0s 106us/sample - loss: 1.0050 - acc: 0.3619 - val_loss: 1.0203 - val_acc: 0.3778\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - 0s 107us/sample - loss: 1.0022 - acc: 0.3810 - val_loss: 1.0173 - val_acc: 0.3778\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - 0s 102us/sample - loss: 0.9994 - acc: 0.3810 - val_loss: 1.0144 - val_acc: 0.4222\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - 0s 97us/sample - loss: 0.9967 - acc: 0.3810 - val_loss: 1.0116 - val_acc: 0.4222\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - 0s 113us/sample - loss: 0.9940 - acc: 0.3810 - val_loss: 1.0088 - val_acc: 0.4222\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - 0s 102us/sample - loss: 0.9913 - acc: 0.3810 - val_loss: 1.0061 - val_acc: 0.4222\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - 0s 119us/sample - loss: 0.9887 - acc: 0.3810 - val_loss: 1.0034 - val_acc: 0.4222\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - 0s 108us/sample - loss: 0.9861 - acc: 0.3810 - val_loss: 1.0008 - val_acc: 0.4222\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - 0s 129us/sample - loss: 0.9834 - acc: 0.3905 - val_loss: 0.9982 - val_acc: 0.4222\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - 0s 107us/sample - loss: 0.9808 - acc: 0.3905 - val_loss: 0.9956 - val_acc: 0.4222\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - 0s 152us/sample - loss: 0.9783 - acc: 0.3905 - val_loss: 0.9930 - val_acc: 0.4222\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - 0s 177us/sample - loss: 0.9757 - acc: 0.3905 - val_loss: 0.9904 - val_acc: 0.4222\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - 0s 156us/sample - loss: 0.9731 - acc: 0.3905 - val_loss: 0.9879 - val_acc: 0.4222\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - 0s 127us/sample - loss: 0.9705 - acc: 0.4000 - val_loss: 0.9854 - val_acc: 0.4222\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - 0s 118us/sample - loss: 0.9681 - acc: 0.4000 - val_loss: 0.9828 - val_acc: 0.4222\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - 0s 151us/sample - loss: 0.9656 - acc: 0.4095 - val_loss: 0.9802 - val_acc: 0.4222\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.9632 - acc: 0.4095 - val_loss: 0.9776 - val_acc: 0.4222\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - 0s 166us/sample - loss: 0.9608 - acc: 0.4095 - val_loss: 0.9750 - val_acc: 0.4222\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - 0s 165us/sample - loss: 0.9585 - acc: 0.4476 - val_loss: 0.9725 - val_acc: 0.4444\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - 0s 161us/sample - loss: 0.9562 - acc: 0.4667 - val_loss: 0.9699 - val_acc: 0.4444\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - 0s 204us/sample - loss: 0.9539 - acc: 0.4762 - val_loss: 0.9675 - val_acc: 0.4667\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - 0s 171us/sample - loss: 0.9517 - acc: 0.4857 - val_loss: 0.9651 - val_acc: 0.4667\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - 0s 134us/sample - loss: 0.9496 - acc: 0.4857 - val_loss: 0.9628 - val_acc: 0.4667\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - 0s 151us/sample - loss: 0.9475 - acc: 0.4952 - val_loss: 0.9605 - val_acc: 0.4667\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - 0s 151us/sample - loss: 0.9455 - acc: 0.4952 - val_loss: 0.9582 - val_acc: 0.4667\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - 0s 136us/sample - loss: 0.9435 - acc: 0.4952 - val_loss: 0.9560 - val_acc: 0.4667\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - 0s 149us/sample - loss: 0.9415 - acc: 0.4952 - val_loss: 0.9538 - val_acc: 0.4889\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - 0s 108us/sample - loss: 0.9396 - acc: 0.4952 - val_loss: 0.9515 - val_acc: 0.5333\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - 0s 140us/sample - loss: 0.9375 - acc: 0.5143 - val_loss: 0.9493 - val_acc: 0.5333\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - 0s 128us/sample - loss: 0.9355 - acc: 0.5143 - val_loss: 0.9470 - val_acc: 0.6000\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - 0s 160us/sample - loss: 0.9335 - acc: 0.5143 - val_loss: 0.9448 - val_acc: 0.6000\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - 0s 140us/sample - loss: 0.9315 - acc: 0.5238 - val_loss: 0.9426 - val_acc: 0.6000\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - 0s 151us/sample - loss: 0.9295 - acc: 0.5333 - val_loss: 0.9403 - val_acc: 0.6000\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - 0s 151us/sample - loss: 0.9274 - acc: 0.5333 - val_loss: 0.9381 - val_acc: 0.6000\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - 0s 116us/sample - loss: 0.9254 - acc: 0.5333 - val_loss: 0.9360 - val_acc: 0.6000\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - 0s 136us/sample - loss: 0.9234 - acc: 0.5333 - val_loss: 0.9339 - val_acc: 0.6000\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.9214 - acc: 0.5333 - val_loss: 0.9319 - val_acc: 0.6000\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - 0s 154us/sample - loss: 0.9196 - acc: 0.5429 - val_loss: 0.9300 - val_acc: 0.6000\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - 0s 171us/sample - loss: 0.9177 - acc: 0.5429 - val_loss: 0.9281 - val_acc: 0.6000\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - 0s 179us/sample - loss: 0.9159 - acc: 0.5619 - val_loss: 0.9262 - val_acc: 0.6222\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - 0s 145us/sample - loss: 0.9141 - acc: 0.5619 - val_loss: 0.9243 - val_acc: 0.6222\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - 0s 142us/sample - loss: 0.9123 - acc: 0.5619 - val_loss: 0.9224 - val_acc: 0.6222\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - 0s 155us/sample - loss: 0.9105 - acc: 0.5619 - val_loss: 0.9206 - val_acc: 0.6222\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - 0s 160us/sample - loss: 0.9088 - acc: 0.5810 - val_loss: 0.9187 - val_acc: 0.6444\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - 0s 169us/sample - loss: 0.9070 - acc: 0.5905 - val_loss: 0.9169 - val_acc: 0.6444\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - 0s 134us/sample - loss: 0.9053 - acc: 0.6000 - val_loss: 0.9151 - val_acc: 0.6444\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - 0s 140us/sample - loss: 0.9036 - acc: 0.6095 - val_loss: 0.9134 - val_acc: 0.6444\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - 0s 137us/sample - loss: 0.9019 - acc: 0.6190 - val_loss: 0.9116 - val_acc: 0.6444\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - 0s 117us/sample - loss: 0.9002 - acc: 0.6190 - val_loss: 0.9097 - val_acc: 0.6444\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - 0s 159us/sample - loss: 0.8986 - acc: 0.6190 - val_loss: 0.9080 - val_acc: 0.6667\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - 0s 125us/sample - loss: 0.8970 - acc: 0.6190 - val_loss: 0.9062 - val_acc: 0.6667\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - 0s 152us/sample - loss: 0.8953 - acc: 0.6190 - val_loss: 0.9044 - val_acc: 0.6667\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.8937 - acc: 0.6190 - val_loss: 0.9026 - val_acc: 0.6667\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - 0s 108us/sample - loss: 0.8922 - acc: 0.6286 - val_loss: 0.9009 - val_acc: 0.6667\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - 0s 108us/sample - loss: 0.8906 - acc: 0.6381 - val_loss: 0.8992 - val_acc: 0.6667\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.8890 - acc: 0.6476 - val_loss: 0.8975 - val_acc: 0.6667\n",
      "Epoch 62/100\n",
      "105/105 [==============================] - 0s 150us/sample - loss: 0.8875 - acc: 0.6476 - val_loss: 0.8959 - val_acc: 0.6667\n",
      "Epoch 63/100\n",
      "105/105 [==============================] - 0s 177us/sample - loss: 0.8860 - acc: 0.6667 - val_loss: 0.8942 - val_acc: 0.6667\n",
      "Epoch 64/100\n",
      "105/105 [==============================] - 0s 139us/sample - loss: 0.8844 - acc: 0.6762 - val_loss: 0.8926 - val_acc: 0.6667\n",
      "Epoch 65/100\n",
      "105/105 [==============================] - 0s 158us/sample - loss: 0.8830 - acc: 0.6952 - val_loss: 0.8910 - val_acc: 0.6667\n",
      "Epoch 66/100\n",
      "105/105 [==============================] - 0s 135us/sample - loss: 0.8815 - acc: 0.7048 - val_loss: 0.8894 - val_acc: 0.6667\n",
      "Epoch 67/100\n",
      "105/105 [==============================] - 0s 158us/sample - loss: 0.8800 - acc: 0.7048 - val_loss: 0.8878 - val_acc: 0.6667\n",
      "Epoch 68/100\n",
      "105/105 [==============================] - 0s 194us/sample - loss: 0.8785 - acc: 0.7048 - val_loss: 0.8862 - val_acc: 0.6889\n",
      "Epoch 69/100\n",
      "105/105 [==============================] - 0s 153us/sample - loss: 0.8770 - acc: 0.7048 - val_loss: 0.8846 - val_acc: 0.6889\n",
      "Epoch 70/100\n",
      "105/105 [==============================] - 0s 144us/sample - loss: 0.8755 - acc: 0.7048 - val_loss: 0.8830 - val_acc: 0.6889\n",
      "Epoch 71/100\n",
      "105/105 [==============================] - 0s 178us/sample - loss: 0.8741 - acc: 0.7048 - val_loss: 0.8814 - val_acc: 0.6889\n",
      "Epoch 72/100\n",
      "105/105 [==============================] - 0s 144us/sample - loss: 0.8726 - acc: 0.7333 - val_loss: 0.8798 - val_acc: 0.6889\n",
      "Epoch 73/100\n",
      "105/105 [==============================] - 0s 203us/sample - loss: 0.8711 - acc: 0.7429 - val_loss: 0.8783 - val_acc: 0.6889\n",
      "Epoch 74/100\n",
      "105/105 [==============================] - 0s 150us/sample - loss: 0.8697 - acc: 0.7429 - val_loss: 0.8767 - val_acc: 0.6889\n",
      "Epoch 75/100\n",
      "105/105 [==============================] - 0s 152us/sample - loss: 0.8682 - acc: 0.7429 - val_loss: 0.8752 - val_acc: 0.6889\n",
      "Epoch 76/100\n",
      "105/105 [==============================] - 0s 148us/sample - loss: 0.8668 - acc: 0.7524 - val_loss: 0.8737 - val_acc: 0.7111\n",
      "Epoch 77/100\n",
      "105/105 [==============================] - 0s 127us/sample - loss: 0.8654 - acc: 0.7524 - val_loss: 0.8722 - val_acc: 0.7111\n",
      "Epoch 78/100\n",
      "105/105 [==============================] - 0s 140us/sample - loss: 0.8640 - acc: 0.7619 - val_loss: 0.8707 - val_acc: 0.7111\n",
      "Epoch 79/100\n",
      "105/105 [==============================] - 0s 100us/sample - loss: 0.8626 - acc: 0.7619 - val_loss: 0.8692 - val_acc: 0.7111\n",
      "Epoch 80/100\n",
      "105/105 [==============================] - 0s 121us/sample - loss: 0.8612 - acc: 0.7619 - val_loss: 0.8677 - val_acc: 0.7111\n",
      "Epoch 81/100\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 0.8598 - acc: 0.7619 - val_loss: 0.8663 - val_acc: 0.7556\n",
      "Epoch 82/100\n",
      "105/105 [==============================] - 0s 115us/sample - loss: 0.8585 - acc: 0.7714 - val_loss: 0.8648 - val_acc: 0.7556\n",
      "Epoch 83/100\n",
      "105/105 [==============================] - 0s 176us/sample - loss: 0.8571 - acc: 0.7714 - val_loss: 0.8634 - val_acc: 0.7556\n",
      "Epoch 84/100\n",
      "105/105 [==============================] - 0s 171us/sample - loss: 0.8558 - acc: 0.7714 - val_loss: 0.8619 - val_acc: 0.7556\n",
      "Epoch 85/100\n",
      "105/105 [==============================] - 0s 141us/sample - loss: 0.8544 - acc: 0.7714 - val_loss: 0.8604 - val_acc: 0.7556\n",
      "Epoch 86/100\n",
      "105/105 [==============================] - 0s 151us/sample - loss: 0.8530 - acc: 0.7714 - val_loss: 0.8590 - val_acc: 0.7556\n",
      "Epoch 87/100\n",
      "105/105 [==============================] - 0s 195us/sample - loss: 0.8516 - acc: 0.7714 - val_loss: 0.8575 - val_acc: 0.7556\n",
      "Epoch 88/100\n",
      "105/105 [==============================] - 0s 174us/sample - loss: 0.8503 - acc: 0.7714 - val_loss: 0.8561 - val_acc: 0.7556\n",
      "Epoch 89/100\n",
      "105/105 [==============================] - 0s 170us/sample - loss: 0.8489 - acc: 0.7714 - val_loss: 0.8548 - val_acc: 0.7778\n",
      "Epoch 90/100\n",
      "105/105 [==============================] - 0s 117us/sample - loss: 0.8476 - acc: 0.7714 - val_loss: 0.8534 - val_acc: 0.7778\n",
      "Epoch 91/100\n",
      "105/105 [==============================] - 0s 157us/sample - loss: 0.8462 - acc: 0.7714 - val_loss: 0.8520 - val_acc: 0.8000\n",
      "Epoch 92/100\n",
      "105/105 [==============================] - 0s 102us/sample - loss: 0.8449 - acc: 0.7714 - val_loss: 0.8507 - val_acc: 0.8222\n",
      "Epoch 93/100\n",
      "105/105 [==============================] - 0s 155us/sample - loss: 0.8436 - acc: 0.7714 - val_loss: 0.8494 - val_acc: 0.8222\n",
      "Epoch 94/100\n",
      "105/105 [==============================] - 0s 152us/sample - loss: 0.8423 - acc: 0.7810 - val_loss: 0.8481 - val_acc: 0.8222\n",
      "Epoch 95/100\n",
      "105/105 [==============================] - 0s 168us/sample - loss: 0.8410 - acc: 0.7810 - val_loss: 0.8468 - val_acc: 0.8222\n",
      "Epoch 96/100\n",
      "105/105 [==============================] - 0s 131us/sample - loss: 0.8397 - acc: 0.7810 - val_loss: 0.8455 - val_acc: 0.8222\n",
      "Epoch 97/100\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.8385 - acc: 0.7810 - val_loss: 0.8441 - val_acc: 0.8222\n",
      "Epoch 98/100\n",
      "105/105 [==============================] - 0s 151us/sample - loss: 0.8371 - acc: 0.7810 - val_loss: 0.8427 - val_acc: 0.8222\n",
      "Epoch 99/100\n",
      "105/105 [==============================] - 0s 132us/sample - loss: 0.8358 - acc: 0.7810 - val_loss: 0.8414 - val_acc: 0.8222\n",
      "Epoch 100/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.8345 - acc: 0.7905 - val_loss: 0.8401 - val_acc: 0.8222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7feecf3f9690>"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=100, batch_size = X_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eSEwm4fvGBDW"
   },
   "source": [
    "### As we can see, Testing accuracy with single output layer was 80%. After adding 2 hidden layers, the Testing accuracy went to 82.22%.\n",
    "### By adding more hidden layers, the classification algorithm will have more decision surfaces and so better chances of classifying the given dataset  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "R6_Internal_Lab_UpdatedTF2_Prices_Iris.ipynb",
   "provenance": [
    {
     "file_id": "1UMMqRmnynHeg1q7B9ZSNlYMTgu50R7T-",
     "timestamp": 1575288906988
    },
    {
     "file_id": "1IwHYQv8vhd7SeqBl6kw9V6nzqW1La-Kv",
     "timestamp": 1575286044599
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
